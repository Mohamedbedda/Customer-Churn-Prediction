Training Logistic Regression...
Best parameters for Logistic Regression: {'model__C': 0.1, 'model__penalty': 'l2'}
Train Accuracy: 78.78 %
Test Accuracy: 77.91 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.81      0.91      0.86      1294
           1       0.62      0.42      0.50       467

    accuracy                           0.78      1761
   macro avg       0.72      0.67      0.68      1761
weighted avg       0.76      0.78      0.76      1761

roc_auc: 0.81
pr_auc:  0.59
Model saved to models/logistic_regression.pkl
#################################################################################
#################################################################################
Training Logistic Regression balanced...
Best parameters for Logistic Regression balanced: {'model__C': 0.01, 'model__penalty': 'l1'}
Train Accuracy: 71.09 %
Test Accuracy: 71.15 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.90      0.69      0.78      1294
           1       0.47      0.78      0.59       467

    accuracy                           0.71      1761
   macro avg       0.68      0.73      0.68      1761
weighted avg       0.78      0.71      0.73      1761

roc_auc: 0.81
pr_auc:  0.60
Model saved to models/logistic_regression_balanced.pkl
#################################################################################
#################################################################################
Training Logistic Regression smote...
Best parameters for Logistic Regression smote: {'pipeline__model__C': 0.01, 'pipeline__model__penalty': 'l2'}
Train Accuracy: 72.32 %
Test Accuracy: 72.40 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.88      0.72      0.79      1294
           1       0.49      0.73      0.58       467

    accuracy                           0.72      1761
   macro avg       0.68      0.73      0.69      1761
weighted avg       0.78      0.72      0.74      1761

roc_auc: 0.81
pr_auc:  0.59
Model saved to models/logistic_regression_smote.pkl
#################################################################################
#################################################################################
Training Logistic Regression adasyn...
Best parameters for Logistic Regression adasyn: {'pipeline__model__C': 100.0, 'pipeline__model__penalty': 'l2'}
Train Accuracy: 70.58 %
Test Accuracy: 71.27 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.90      0.69      0.78      1294
           1       0.47      0.78      0.59       467

    accuracy                           0.71      1761
   macro avg       0.69      0.73      0.68      1761
weighted avg       0.79      0.71      0.73      1761

roc_auc: 0.81
pr_auc:  0.59
Model saved to models/logistic_regression_adasyn.pkl
#################################################################################
#################################################################################
#################################################################################
Training Random Forest...
Best parameters for Random Forest: {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 200}
Train Accuracy: 87.58 %
Test Accuracy: 79.95 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.84      0.91      0.87      1294
           1       0.66      0.51      0.57       467

    accuracy                           0.80      1761
   macro avg       0.75      0.71      0.72      1761
weighted avg       0.79      0.80      0.79      1761

roc_auc: 0.84
pr_auc:  0.65
Model saved to models/random_forest.pkl
#################################################################################
#################################################################################
Training Random Forest balanced...
Best parameters for Random Forest balanced: {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 200}
Train Accuracy: 86.07 %
Test Accuracy: 77.51 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.89      0.80      0.84      1294
           1       0.56      0.72      0.63       467

    accuracy                           0.78      1761
   macro avg       0.72      0.76      0.73      1761
weighted avg       0.80      0.78      0.78      1761

roc_auc: 0.84
pr_auc:  0.64
Model saved to models/random_forest_balanced.pkl
#################################################################################
#################################################################################
Training Random Forest smote...
Best parameters for Random Forest smote: {'model__max_depth': 5, 'model__max_features': 'sqrt', 'model__n_estimators': 100}
Train Accuracy: 76.88 %
Test Accuracy: 76.66 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.90      0.77      0.83      1294
           1       0.54      0.75      0.63       467

    accuracy                           0.77      1761
   macro avg       0.72      0.76      0.73      1761
weighted avg       0.80      0.77      0.78      1761

roc_auc: 0.84
pr_auc:  0.62
Model saved to models/random_forest_smote.pkl
#################################################################################
#################################################################################
Training Random Forest adasyn...
/home/med/.local/lib/python3.10/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast
  _data = np.array(data, dtype=dtype, copy=copy,
Best parameters for Random Forest adasyn: {'model__max_depth': 10, 'model__max_features': 'log2', 'model__n_estimators': 100}
Train Accuracy: 83.87 %
Test Accuracy: 77.51 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.88      0.81      0.84      1294
           1       0.56      0.68      0.62       467

    accuracy                           0.78      1761
   macro avg       0.72      0.75      0.73      1761
weighted avg       0.79      0.78      0.78      1761

roc_auc: 0.83
pr_auc:  0.62
Model saved to models/random_forest_adasyn.pkl
#################################################################################
#################################################################################
#################################################################################
Training XGBoost...
Best parameters for XGBoost: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 300}
Train Accuracy: 82.70 %
Test Accuracy: 80.35 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.83      0.92      0.87      1294
           1       0.68      0.49      0.57       467

    accuracy                           0.80      1761
   macro avg       0.76      0.70      0.72      1761
weighted avg       0.79      0.80      0.79      1761

roc_auc: 0.85
pr_auc:  0.65
Model saved to models/xgboost.pkl
#################################################################################
#################################################################################
Training XGBoost balanced...
Best parameters for XGBoost balanced: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200}
Train Accuracy: 77.72 %
Test Accuracy: 75.64 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.91      0.74      0.82      1294
           1       0.53      0.80      0.64       467

    accuracy                           0.76      1761
   macro avg       0.72      0.77      0.73      1761
weighted avg       0.81      0.76      0.77      1761

roc_auc: 0.85
pr_auc:  0.65
Model saved to models/xgboost_balanced.pkl
#################################################################################
#################################################################################
Training XGBoost smote...
Best parameters for XGBoost smote: {'model__learning_rate': 0.01, 'model__max_depth': 5, 'model__n_estimators': 200}
Train Accuracy: 79.67 %
Test Accuracy: 77.00 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.88      0.80      0.84      1294
           1       0.55      0.70      0.62       467

    accuracy                           0.77      1761
   macro avg       0.72      0.75      0.73      1761
weighted avg       0.79      0.77      0.78      1761

roc_auc: 0.84
pr_auc:  0.63
Model saved to models/xgboost_smote.pkl
#################################################################################
#################################################################################
Training XGBoost adasyn...
Best parameters for XGBoost adasyn: {'model__learning_rate': 0.01, 'model__max_depth': 5, 'model__n_estimators': 300}
Train Accuracy: 79.65 %
Test Accuracy: 77.29 %
--------------------------------------------------
Test Classification Report:

              precision    recall  f1-score   support

           0       0.88      0.80      0.84      1294
           1       0.56      0.69      0.62       467

    accuracy                           0.77      1761
   macro avg       0.72      0.75      0.73      1761
weighted avg       0.79      0.77      0.78      1761

roc_auc: 0.84
pr_auc:  0.63
Model saved to models/xgboost_adasyn.pkl