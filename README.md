# ğŸ“Š Customer Churn Prediction

This repository contains a **machine learning project** for predicting customer churn in the telecom sector, using the [Telco Customer Churn dataset](https://www.kaggle.com/datasets/blastchar/telco-customer-churn) from Kaggle.
It demonstrates an **end-to-end ML workflow**, from data preparation to model evaluation, with integrated **experiment tracking using MLflow**.



---

## ğŸš€ Features

* ğŸ›  **Data preparation** (cleaning, encoding, normalization)
* ğŸ““ **Exploratory Data Analysis (EDA)** in Jupyter notebook
* ğŸ¤– **Multiple models implemented**: Logistic Regression, Random Forest, XGBoost
* ğŸ› **Hyperparameter fine-tuning** with grid search / cross-validation
* âš–ï¸ **Handling class imbalance** using weighted loss and resampling methods (SMOTE, ADASYN)
* ğŸ“ˆ **Evaluation helpers**: metrics, confusion matrix
* ğŸ”¬ **Experiment tracking with MLflow**

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ data/                     # Raw and processed datasets
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ EDA.ipynb             # Exploratory data analysis
â”œâ”€â”€ reports/                  # Evaluation outputs (metrics, plots)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ base_trainer.py       # Abstract class for models training
â”‚   â”œâ”€â”€ config.py             # Paths, hyperparameters, configuration
â”‚   â”œâ”€â”€ data_prep.py          # Data loading & preprocessing
â”‚   â”œâ”€â”€ evaluation.py         # Metrics and reporting helpers
â”‚   â”œâ”€â”€ logistic_regression.py
â”‚   â”œâ”€â”€ random_forest.py
â”‚   â””â”€â”€ xgboost.py
â”œâ”€â”€ train.py                  # Main training entry point
â”œâ”€â”€ mlflow_train.py           # Training with MLflow experiment tracking
â””â”€â”€ requirements.txt          # Dependencies
```

---

## âš¡ Quickstart

### 1. Create and activate a virtual environment

**macOS/Linux**

```bash
python -m venv .venv
source .venv/bin/activate
```

**Windows (PowerShell)**

```bash
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

---

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

---

### 3. Train a model
Run a model directly (`<method>` choices: `lr` = Logistic Regression, `rf` = Random Forest, `xgb` = XGBoost).

You can handle class imbalance with different methods (optional argument `<method>`):  
- `balanced` â†’ applies **class weights** (weighted loss).  
- `smote` â†’ applies **Synthetic Minority Oversampling Technique**.  
- `adasyn` â†’ applies **Adaptive Synthetic Sampling**.  

If `<method>` is omitted, the model is trained without class balancing.

**Default training (no tracking):**
```bash
python train.py <model> <method>
```

**With MLflow experiment tracking:**
For full experiment logging (parameters, metrics, artifacts, confusion matrix, saved model):

```bash
python mlflow_train.py <model> <method>
```

### 4. Evaluate results

* The evaluation results are printed on the terminal, and the confusion matrix plots are saved under `reports/`.
* You can also explore the results interactively in the **MLflow UI**:

```bash
mlflow ui
```

ğŸ‘‰ Open [http://127.0.0.1:5000](http://127.0.0.1:5000) to compare runs.

---
